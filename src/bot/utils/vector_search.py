"""C9. Vector Search 2.0 ‚Äî –ø–æ–∏—Å–∫ –ø–æ —Å–º—ã—Å–ª—É –≤ Consult Log –∏ —Å—Ç–∞—Ç—å—è—Ö.

–†–µ–∞–ª–∏–∑—É–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫: –µ—Å–ª–∏ –ø–æ—Ö–æ–∂–∏–π –≤–æ–ø—Ä–æ—Å —É–∂–µ —Ä–µ—à–∞–ª—Å—è,
AI –±–µ—Ä—ë—Ç –∑–∞ –æ—Å–Ω–æ–≤—É —Å—Ç–∞—Ä—ã–π –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç.

C8. Practice Area AI ‚Äî —É–∑–∫–æ—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ –æ—Ç—Ä–∞—Å–ª–∏.

C10. QA Audit AI ‚Äî –µ–∂–µ–Ω–µ–¥–µ–ª—å–Ω—ã–π –∞—É–¥–∏—Ç –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–æ–≤.
"""

import asyncio
import logging
import re
from collections import defaultdict
from typing import Optional

logger = logging.getLogger(__name__)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#  C9: Vector Search 2.0 (TF-IDF + cosine similarity)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

# –°—Ç–æ–ø-—Å–ª–æ–≤–∞ (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ)
_STOP_WORDS = {
    "–∏", "–≤", "–Ω–∞", "—Å", "–ø–æ", "–∏–∑", "–∫", "—É", "–æ", "–∑–∞", "–æ—Ç", "–¥–æ",
    "–Ω–µ", "—á—Ç–æ", "–∫–∞–∫", "—ç—Ç–æ", "—Ç–æ", "–≤—Å–µ", "–µ–≥–æ", "–Ω–æ", "–¥–∞", "–º—ã",
    "–æ–Ω", "–æ–Ω–∞", "–æ–Ω–∏", "–≤—ã", "–º–Ω–µ", "–Ω–∞—Å", "–≤–∞—Å", "–µ–º—É", "–µ–π", "–∏—Ö",
    "–±—ã—Ç—å", "–±—ã–ª", "–±—ã–ª–∞", "–±—ã–ª–∏", "–±—É–¥–µ—Ç", "–µ—Å—Ç—å", "–ª–∏", "–∂–µ", "–±—ã",
    "–¥–ª—è", "–ø—Ä–∏", "—Ç–∞–∫", "–µ—â—ë", "–µ—â–µ", "—É–∂–µ", "—Ç–æ–∂–µ", "–∏–ª–∏", "–∞",
    "–≤–∞—à", "–Ω–∞—à", "—ç—Ç–æ—Ç", "—Ç–æ—Ç", "–º–æ–π", "—Å–≤–æ–π", "–∫–∞–∫–æ–π", "–∫–æ—Ç–æ—Ä—ã–π",
    "–Ω—É–∂–Ω–æ", "–º–æ–∂–µ—Ç", "–º–æ–∂–Ω–æ", "–æ—á–µ–Ω—å", "—Ç–æ–ª—å–∫–æ", "–¥–∞–∂–µ", "—á–µ—Ä–µ–∑",
    "–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ", "–¥–æ–±—Ä—ã–π", "–¥–µ–Ω—å", "–ø–æ–∂–∞–ª—É–π—Å—Ç–∞", "—Å–ø–∞—Å–∏–±–æ", "–ø–æ–¥—Å–∫–∞–∂–∏—Ç–µ",
}


def _tokenize(text: str) -> list[str]:
    """–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å —É–¥–∞–ª–µ–Ω–∏–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤."""
    words = re.findall(r'[–∞-—è—ëa-z0-9]+', text.lower())
    return [w for w in words if w not in _STOP_WORDS and len(w) > 2]


def _compute_tfidf(docs: list[list[str]]) -> tuple[list[dict], dict]:
    """–í—ã—á–∏—Å–ª—è–µ—Ç TF-IDF –¥–ª—è —Å–ø–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."""
    import math

    doc_freq = defaultdict(int)
    for doc in docs:
        seen = set()
        for w in doc:
            if w not in seen:
                doc_freq[w] += 1
                seen.add(w)

    n_docs = len(docs)
    idf = {}
    for word, df in doc_freq.items():
        idf[word] = math.log(n_docs / (df + 1)) + 1

    tfidf_docs = []
    for doc in docs:
        tf = defaultdict(int)
        for w in doc:
            tf[w] += 1
        max_tf = max(tf.values()) if tf else 1
        tfidf = {}
        for w, count in tf.items():
            tfidf[w] = (count / max_tf) * idf.get(w, 1)
        tfidf_docs.append(tfidf)

    return tfidf_docs, idf


def _cosine_similarity(vec_a: dict, vec_b: dict) -> float:
    """–ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –¥–≤—É–º—è TF-IDF –≤–µ–∫—Ç–æ—Ä–∞–º–∏."""
    import math

    common = set(vec_a.keys()) & set(vec_b.keys())
    if not common:
        return 0.0

    dot = sum(vec_a[w] * vec_b[w] for w in common)
    norm_a = math.sqrt(sum(v ** 2 for v in vec_a.values()))
    norm_b = math.sqrt(sum(v ** 2 for v in vec_b.values()))

    if norm_a == 0 or norm_b == 0:
        return 0.0

    return dot / (norm_a * norm_b)


# In-memory –∏–Ω–¥–µ–∫—Å
_index: list[dict] = []  # [{text, tokens, tfidf, source, metadata}]
_index_built = False


def build_index(entries: list[dict]) -> int:
    """–°—Ç—Ä–æ–∏—Ç –ø–æ–∏—Å–∫–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å –∏–∑ –∑–∞–ø–∏—Å–µ–π.

    Args:
        entries: [{"text": str, "source": str, "metadata": dict}]

    Returns:
        –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π.
    """
    global _index, _index_built

    if not entries:
        return 0

    docs = []
    for entry in entries:
        text = entry.get("text", "")
        tokens = _tokenize(text)
        if tokens:
            docs.append(tokens)
            _index.append({
                "text": text[:1000],
                "tokens": tokens,
                "source": entry.get("source", ""),
                "metadata": entry.get("metadata", {}),
            })

    if docs:
        tfidf_docs, _ = _compute_tfidf(docs)
        for i, tfidf in enumerate(tfidf_docs):
            if i < len(_index):
                _index[i]["tfidf"] = tfidf

    _index_built = True
    logger.info("Vector index built: %d entries", len(_index))
    return len(_index)


def search_similar(query: str, top_k: int = 5, min_score: float = 0.1) -> list[dict]:
    """–ò—â–µ—Ç –ø–æ—Ö–æ–∂–∏–µ –∑–∞–ø–∏—Å–∏ –ø–æ —Å–º—ã—Å–ª—É.

    Returns:
        [{text, source, score, metadata}]
    """
    if not _index:
        return []

    query_tokens = _tokenize(query)
    if not query_tokens:
        return []

    # –°—Ç—Ä–æ–∏–º TF –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
    from collections import Counter
    tf = Counter(query_tokens)
    max_tf = max(tf.values())
    query_vec = {w: count / max_tf for w, count in tf.items()}

    # –°—á–∏—Ç–∞–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ —Å–æ –≤—Å–µ–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
    results = []
    for entry in _index:
        tfidf = entry.get("tfidf", {})
        if not tfidf:
            continue
        score = _cosine_similarity(query_vec, tfidf)
        if score >= min_score:
            results.append({
                "text": entry["text"],
                "source": entry["source"],
                "score": round(score, 4),
                "metadata": entry.get("metadata", {}),
            })

    results.sort(key=lambda x: x["score"], reverse=True)
    return results[:top_k]


async def search_consult_history(
    query: str, google=None, cache=None, top_k: int = 3,
) -> list[dict]:
    """–ò—â–µ—Ç –ø–æ—Ö–æ–∂–∏–µ –≤–æ–ø—Ä–æ—Å—ã –≤ –∏—Å—Ç–æ—Ä–∏–∏ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–π.

    –ó–∞–≥—Ä—É–∂–∞–µ—Ç Consult Log –∏ —Å—Ç–∞—Ç—å–∏, —Å—Ç—Ä–æ–∏—Ç –∏–Ω–¥–µ–∫—Å (–ø—Ä–∏ –ø–µ—Ä–≤–æ–º –≤—ã–∑–æ–≤–µ)
    –∏ –∏—â–µ—Ç –ø–æ—Ö–æ–∂–∏–µ –∑–∞–ø–∏—Å–∏.
    """
    global _index_built

    # –ü–µ—Ä–µ—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å –µ—Å–ª–∏ –æ–Ω –ø—É—Å—Ç–æ–π –∏ Google –¥–æ—Å—Ç—É–ø–µ–Ω
    if not _index_built and google:
        try:
            entries = []

            # Consult Log
            consult_log = await google.get_consult_log(300)
            for entry in consult_log:
                q = entry.get("question", "")
                a = entry.get("answer", "")
                if q:
                    entries.append({
                        "text": f"–í–æ–ø—Ä–æ—Å: {q}\n–û—Ç–≤–µ—Ç: {a}",
                        "source": "consult_log",
                        "metadata": {"user_id": entry.get("user_id", "")},
                    })

            # –°—Ç–∞—Ç—å–∏
            articles = await google.get_articles_list(limit=50)
            for art in articles:
                title = art.get("title", "")
                content = art.get("content", art.get("description", ""))
                if title:
                    entries.append({
                        "text": f"{title}\n{content[:500]}",
                        "source": "article",
                        "metadata": {"title": title},
                    })

            if entries:
                build_index(entries)
        except Exception as e:
            logger.warning("Failed to build vector index: %s", e)

    return search_similar(query, top_k=top_k)


def format_search_results(results: list[dict]) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è AI."""
    if not results:
        return ""

    parts = ["üìö –ü–û–•–û–ñ–ò–ï –ü–†–ï–¶–ï–î–ï–ù–¢–´ –ò–ó –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô:"]
    for i, r in enumerate(results, 1):
        source = "üìù –ö–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è" if r["source"] == "consult_log" else "üì∞ –°—Ç–∞—Ç—å—è"
        parts.append(f"\n{i}. {source} (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {r['score']:.0%}):")
        parts.append(f"   {r['text'][:300]}")

    return "\n".join(parts)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#  C8: Practice Area AI
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

PRACTICE_AREAS = {
    "tax": {
        "name": "–ù–∞–ª–æ–≥–æ–≤–æ–µ –ø—Ä–∞–≤–æ",
        "keywords": ["–Ω–∞–ª–æ–≥", "–∫–ø–Ω", "–Ω–¥—Å", "–∏–ø–Ω", "–¥–µ–∫–ª–∞—Ä–∞—Ü", "–±—é–¥–∂–µ—Ç", "—Ñ–∏—Å–∫–∞–ª", "–Ω–∞–ª–æ–≥–æ–≤–∞—è"],
        "context": (
            "–°–ü–ï–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø: –ù–∞–ª–æ–≥–æ–≤–æ–µ –ø—Ä–∞–≤–æ –†–ö.\n"
            "–ö–ª—é—á–µ–≤—ã–µ –ù–ü–ê: –ö–æ–¥–µ–∫—Å –†–ö ¬´–û –Ω–∞–ª–æ–≥–∞—Ö –∏ –¥—Ä—É–≥–∏—Ö –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–ª–∞—Ç–µ–∂–∞—Ö¬ª (–ù–ö –†–ö).\n"
            "–û–±—Ä–∞—â–∞–π –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞: —Å—Ç–∞–≤–∫–∏ –Ω–∞–ª–æ–≥–æ–≤, —Å—Ä–æ–∫–∏ —Å–¥–∞—á–∏ –¥–µ–∫–ª–∞—Ä–∞—Ü–∏–π, "
            "–Ω–∞–ª–æ–≥–æ–≤—ã–µ –ª—å–≥–æ—Ç—ã (–°–≠–ó, IT-–ø–∞—Ä–∫, –ú–§–¶–ê), —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä—Ç–Ω–æ–µ —Ü–µ–Ω–æ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ.\n"
            "–ü—Ä–∏ –æ—Ç–≤–µ—Ç–µ —É–∫–∞–∑—ã–≤–∞–π –ö–û–ù–ö–†–ï–¢–ù–´–ï —Å—Ç–∞—Ç—å–∏ –ù–ö –†–ö."
        ),
    },
    "it_aifc": {
        "name": "IT-–ø—Ä–∞–≤–æ –∏ –ú–§–¶–ê",
        "keywords": ["–º—Ñ—Ü–∞", "aifc", "it", "—Å—Ç–∞—Ä—Ç–∞–ø", "—Ü–∏—Ñ—Ä–æ–≤", "–¥–∞–Ω–Ω—ã–µ", "–ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω", "gdpr"],
        "context": (
            "–°–ü–ï–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø: IT-–ø—Ä–∞–≤–æ, –ø—Ä–∞–≤–æ –ú–§–¶–ê (–ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–π —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π —Ü–µ–Ω—Ç—Ä –ê—Å—Ç–∞–Ω–∞).\n"
            "–ö–ª—é—á–µ–≤—ã–µ –ù–ü–ê: –ö–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏–æ–Ω–Ω—ã–π –∑–∞–∫–æ–Ω –æ –ú–§–¶–ê, AIFC Regulations, "
            "AIFC Data Protection Regulations 2020, Companies Regulations 2017.\n"
            "–í –ú–§–¶–ê –¥–µ–π—Å—Ç–≤—É–µ—Ç –ê–ù–ì–õ–ò–ô–°–ö–û–ï –û–ë–©–ï–ï –ü–†–ê–í–û. –£–∫–∞–∑—ã–≤–∞–π —ç—Ç–æ.\n"
            "–î–ª—è IT: –ó–∞–∫–æ–Ω –æ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –†–ö, –ó–∞–∫–æ–Ω –æ–± –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∑–∞—Ü–∏–∏."
        ),
    },
    "corporate": {
        "name": "–ö–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–µ –ø—Ä–∞–≤–æ",
        "keywords": ["—Ç–æ–æ", "–∞–æ", "—É—á–∞—Å—Ç–Ω–∏–∫", "—É—á—Ä–µ–¥–∏—Ç–µ–ª—å", "—É—Å—Ç–∞–≤", "–¥–æ–ª—è", "–∞–∫—Ü–∏", "–¥–∏–≤–∏–¥–µ–Ω–¥"],
        "context": (
            "–°–ü–ï–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø: –ö–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–µ –ø—Ä–∞–≤–æ –†–ö.\n"
            "–ö–ª—é—á–µ–≤—ã–µ –ù–ü–ê: –ó–∞–∫–æ–Ω –æ –¢–û–û, –ó–∞–∫–æ–Ω –æ–± –ê–û, –ì–ö –†–ö (–ß–∞—Å—Ç—å –æ–±—â–∞—è).\n"
            "–û–±—Ä–∞—â–∞–π –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞: —É—Å—Ç–∞–≤–Ω—ã–π –∫–∞–ø–∏—Ç–∞–ª, –¥–æ–ª–∏ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤, "
            "–≤—ã—Ö–æ–¥ —É—á–∞—Å—Ç–Ω–∏–∫–∞ (—Å—Ç. 69 –ó–∞–∫–æ–Ω–∞ –æ –¢–û–û), —Ä–µ–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è, –ª–∏–∫–≤–∏–¥–∞—Ü–∏—è."
        ),
    },
    "labor": {
        "name": "–¢—Ä—É–¥–æ–≤–æ–µ –ø—Ä–∞–≤–æ",
        "keywords": ["—Ä–∞–±–æ—Ç–Ω–∏–∫", "—Ä–∞–±–æ—Ç–æ–¥–∞—Ç–µ–ª—å", "—Ç—Ä—É–¥–æ–≤", "—É–≤–æ–ª—å", "–∑–∞—Ä–ø–ª–∞—Ç", "–æ—Ç–ø—É—Å–∫", "–±–æ–ª—å–Ω–∏—á–Ω"],
        "context": (
            "–°–ü–ï–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø: –¢—Ä—É–¥–æ–≤–æ–µ –ø—Ä–∞–≤–æ –†–ö.\n"
            "–ö–ª—é—á–µ–≤—ã–µ –ù–ü–ê: –¢—Ä—É–¥–æ–≤–æ–π –∫–æ–¥–µ–∫—Å –†–ö (–¢–ö –†–ö).\n"
            "–ö–ª—é—á–µ–≤—ã–µ —Å—Ç–∞—Ç—å–∏: —Å—Ç. 28 (—Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –¢–î), —Å—Ç. 49-56 (–ø—Ä–µ–∫—Ä–∞—â–µ–Ω–∏–µ), "
            "—Å—Ç. 65 (–¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞—Ä–Ω—ã–µ), —Å—Ç. 87-93 (–æ—Ç–ø—É—Å–∫–∞), —Å—Ç. 101 (–æ–ø–ª–∞—Ç–∞ —Ç—Ä—É–¥–∞).\n"
            "–ù–ü –í–° –†–ö ‚Ññ1 –æ—Ç 28.11.2024 ‚Äî –≤–∞–∂–Ω–æ –¥–ª—è –∞—Ç—Ç–µ—Å—Ç–∞—Ü–∏–∏."
        ),
    },
    "litigation": {
        "name": "–°—É–¥–µ–±–Ω—ã–µ —Å–ø–æ—Ä—ã",
        "keywords": ["—Å—É–¥", "–∏—Å–∫", "–∏—Å—Ç–µ—Ü", "–æ—Ç–≤–µ—Ç—á–∏–∫", "–∞–ø–µ–ª–ª—è—Ü", "–∫–∞—Å—Å–∞—Ü", "–∞—Ä–±–∏—Ç—Ä–∞–∂", "–≤–∑—ã—Å–∫–∞–Ω"],
        "context": (
            "–°–ü–ï–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø: –°—É–¥–µ–±–Ω—ã–µ —Å–ø–æ—Ä—ã –∏ –∞—Ä–±–∏—Ç—Ä–∞–∂.\n"
            "–ö–ª—é—á–µ–≤—ã–µ –ù–ü–ê: –ì–ü–ö –†–ö, –ó–∞–∫–æ–Ω –æ–± –∞—Ä–±–∏—Ç—Ä–∞–∂–µ, –ó–∞–∫–æ–Ω –æ–± –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–º –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ.\n"
            "–£–∫–∞–∑—ã–≤–∞–π: —Å—Ä–æ–∫–∏ –∏—Å–∫–æ–≤–æ–π –¥–∞–≤–Ω–æ—Å—Ç–∏ (3 –≥–æ–¥–∞ –æ–±—â–∏–π ‚Äî —Å—Ç. 178 –ì–ö –†–ö), "
            "–ø–æ–¥—Å—É–¥–Ω–æ—Å—Ç—å, —Ä–∞–∑–º–µ—Ä –≥–æ—Å–ø–æ—à–ª–∏–Ω—ã, –ø–æ—Ä—è–¥–æ–∫ –æ–±–∂–∞–ª–æ–≤–∞–Ω–∏—è."
        ),
    },
    "ip": {
        "name": "–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å",
        "keywords": ["–∞–≤—Ç–æ—Ä—Å–∫", "–ø–∞—Ç–µ–Ω—Ç", "—Ç–æ–≤–∞—Ä–Ω—ã–π –∑–Ω–∞–∫", "–±—Ä–µ–Ω–¥", "–ª–∏—Ü–µ–Ω–∑–∏", "—Ñ—Ä–∞–Ω—à–∏–∑"],
        "context": (
            "–°–ü–ï–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø: –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å.\n"
            "–ö–ª—é—á–µ–≤—ã–µ –ù–ü–ê: –ö–Ω–∏–≥–∞ 5 –ì–ö –†–ö, –ü–∞—Ç–µ–Ω—Ç–Ω—ã–π –∑–∞–∫–æ–Ω –†–ö, "
            "–ó–∞–∫–æ–Ω –æ —Ç–æ–≤–∞—Ä–Ω—ã—Ö –∑–Ω–∞–∫–∞—Ö.\n"
            "–û–±—Ä–∞—â–∞–π –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞: —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—é, —Å—Ä–æ–∫–∏ –æ—Ö—Ä–∞–Ω—ã, –ª–∏—Ü–µ–Ω–∑–∏–æ–Ω–Ω—ã–µ –¥–æ–≥–æ–≤–æ—Ä—ã."
        ),
    },
}


def detect_practice_area(question: str) -> list[dict]:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ–±–ª–∞—Å—Ç—å –ø—Ä–∞–≤–∞ –ø–æ –≤–æ–ø—Ä–æ—Å—É.

    Returns:
        –°–ø–∏—Å–æ–∫ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –æ–±–ª–∞—Å—Ç–µ–π [{name, context}].
    """
    q_lower = question.lower()
    matched = []

    for area_id, area in PRACTICE_AREAS.items():
        score = 0
        for kw in area["keywords"]:
            if kw in q_lower:
                score += 1

        if score > 0:
            matched.append({
                "id": area_id,
                "name": area["name"],
                "context": area["context"],
                "score": score,
            })

    matched.sort(key=lambda x: x["score"], reverse=True)
    return matched[:2]  # –ú–∞–∫—Å 2 –æ–±–ª–∞—Å—Ç–∏


def get_practice_context(question: str) -> str:
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ –æ–±–ª–∞—Å—Ç–∏ –ø—Ä–∞–≤–∞.

    Returns:
        –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è AI.
    """
    areas = detect_practice_area(question)
    if not areas:
        return ""

    parts = ["üéØ –°–ü–ï–¶–ò–ê–õ–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –ö–û–ù–¢–ï–ö–°–¢:"]
    for area in areas:
        parts.append(f"\n[{area['name']}]")
        parts.append(area["context"])

    return "\n".join(parts)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#  C10: QA Audit AI
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê


async def run_qa_audit(google=None, cache=None, bot=None) -> str:
    """–ï–∂–µ–Ω–µ–¥–µ–ª—å–Ω—ã–π AI-–∞—É–¥–∏—Ç –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–æ–≤.

    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç 10 —Å–ª—É—á–∞–π–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤ —Å –Ω–∏–∑–∫–æ–π –æ—Ü–µ–Ω–∫–æ–π
    –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á—ë—Ç —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏.

    Returns:
        –û—Ç—á—ë—Ç –≤ HTML –¥–ª—è Telegram.
    """
    from src.bot.utils.ai_client import get_orchestrator
    import random

    ai = get_orchestrator()

    # –°–æ–±–∏—Ä–∞–µ–º –¥–∏–∞–ª–æ–≥–∏ —Å –Ω–∏–∑–∫–æ–π –æ—Ü–µ–Ω–∫–æ–π
    low_rated = []
    try:
        from src.database.models import async_session, FeedbackScore
        from sqlalchemy import select

        async with async_session() as session:
            result = await session.execute(
                select(FeedbackScore)
                .where(FeedbackScore.score <= 3)
                .order_by(FeedbackScore.created_at.desc())
                .limit(50)
            )
            low_scores = result.scalars().all()

        # –î–æ–ø–æ–ª–Ω—è–µ–º –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ Consult Log
        if google and low_scores:
            consult_log = await google.get_consult_log(200)
            for score_entry in low_scores:
                for cl in consult_log:
                    if str(cl.get("user_id", "")) == str(score_entry.user_id):
                        low_rated.append({
                            "user_id": score_entry.user_id,
                            "score": score_entry.score,
                            "question": cl.get("question", ""),
                            "answer": cl.get("answer", ""),
                        })
                        break

    except Exception as e:
        logger.warning("QA Audit: failed to get low-rated dialogues: %s", e)

    # Fallback: –±–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –∑–∞–ø–∏—Å–∏ –∏–∑ Consult Log
    if not low_rated and google:
        try:
            consult_log = await google.get_consult_log(50)
            low_rated = [
                {
                    "user_id": cl.get("user_id", ""),
                    "score": "N/A",
                    "question": cl.get("question", ""),
                    "answer": cl.get("answer", ""),
                }
                for cl in consult_log[:20]
            ]
        except Exception:
            pass

    if not low_rated:
        return "‚ÑπÔ∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞—É–¥–∏—Ç–∞ –∫–∞—á–µ—Å—Ç–≤–∞."

    # –ë–µ—Ä—ë–º 10 —Å–ª—É—á–∞–π–Ω—ã—Ö
    sample = random.sample(low_rated, min(10, len(low_rated)))

    dialogues_text = ""
    for i, d in enumerate(sample, 1):
        dialogues_text += (
            f"\n–î–∏–∞–ª–æ–≥ {i} (–æ—Ü–µ–Ω–∫–∞: {d['score']}):\n"
            f"  –í–æ–ø—Ä–æ—Å: {d['question'][:200]}\n"
            f"  –û—Ç–≤–µ—Ç: {d['answer'][:200]}\n"
        )

    prompt = (
        "–¢—ã ‚Äî QA-–∞—É–¥–∏—Ç–æ—Ä —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–π —Ñ–∏—Ä–º—ã SOLIS Partners.\n\n"
        "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–∏ 10 –¥–∏–∞–ª–æ–≥–æ–≤ –∏ –Ω–∞–ø–∏—à–∏ –æ—Ç—á—ë—Ç:\n\n"
        f"{dialogues_text}\n\n"
        "–§–û–†–ú–ê–¢ –û–¢–ß–Å–¢–ê:\n"
        "1. üìä <b>–û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞</b> (–æ—Ç 1 –¥–æ 10)\n"
        "2. üî¥ <b>–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã</b> (–æ—à–∏–±–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –∏—Å–ø—Ä–∞–≤–∏—Ç—å –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ)\n"
        "3. üü° <b>–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é –ø—Ä–æ–º–ø—Ç–æ–≤</b>\n"
        "4. üü¢ <b>–ß—Ç–æ —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç</b>\n"
        "5. üìù <b>–ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ —Å–∏—Å—Ç–µ–º–Ω–æ–º –ø—Ä–æ–º–ø—Ç–µ</b>\n\n"
        "–ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º. –ò—Å–ø–æ–ª—å–∑—É–π HTML –¥–ª—è Telegram."
    )

    try:
        report = await ai.call_with_fallback(
            prompt,
            "–¢—ã ‚Äî –¥–∏—Ä–µ–∫—Ç–æ—Ä –ø–æ –∫–∞—á–µ—Å—Ç–≤—É —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–π —Ñ–∏—Ä–º—ã. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ.",
            primary="openai", max_tokens=2048, temperature=0.4,
        )
    except Exception as e:
        logger.error("QA Audit AI failed: %s", e)
        report = f"‚ùå AI-–∞—É–¥–∏—Ç –Ω–µ —É–¥–∞–ª—Å—è: {e}"

    header = (
        "üìä <b>–ï–∂–µ–Ω–µ–¥–µ–ª—å–Ω—ã–π –∞—É–¥–∏—Ç –∫–∞—á–µ—Å—Ç–≤–∞</b>\n"
        f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {len(sample)} –¥–∏–∞–ª–æ–≥–æ–≤\n\n"
    )

    return header + report


async def scheduled_qa_audit(bot=None, google=None, cache=None) -> None:
    """Scheduled –∑–∞–¥–∞—á–∞ –¥–ª—è –µ–∂–µ–Ω–µ–¥–µ–ª—å–Ω–æ–≥–æ QA –∞—É–¥–∏—Ç–∞."""
    from src.config import settings

    report = await run_qa_audit(google=google, cache=cache, bot=bot)

    if bot:
        try:
            if len(report) > 4000:
                for i in range(0, len(report), 4000):
                    await bot.send_message(settings.ADMIN_ID, report[i:i+4000])
            else:
                await bot.send_message(settings.ADMIN_ID, report)
        except Exception as e:
            logger.error("QA report send failed: %s", e)
